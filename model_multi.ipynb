{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbFkUxA-OJOr",
        "outputId": "ba78dd43-7822-4beb-f96f-c93501ac3f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Using device: cpu\n",
            "✅ API publique ngrok : NgrokTunnel: \"https://87d3-34-150-210-96.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n"
          ]
        }
      ],
      "source": [
        "# Installer les dépendances (à lancer une seule fois dans Colab)\n",
        "!pip install flask flask-ngrok pyngrok transformers pillow torch torchvision torchaudio opencv-python moviepy gtts git+https://github.com/openai/whisper.git --quiet\n",
        "\n",
        "# Import des modules nécessaires\n",
        "from flask import Flask, request, send_file, jsonify\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
        "import whisper\n",
        "from moviepy.editor import VideoFileClip, ImageSequenceClip, AudioFileClip\n",
        "from gtts import gTTS\n",
        "\n",
        "# Création de l'app Flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Choix du device GPU si disponible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Chargement des modèles au démarrage (uniquement une fois)\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "def extract_frames(video_path, fps_extract=1):\n",
        "    os.makedirs(\"frames\", exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    count, saved = 0, 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % fps == 0:\n",
        "            path = f\"frames/frame_{saved:04d}.jpg\"\n",
        "            cv2.imwrite(path, frame)\n",
        "            saved += 1\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return saved\n",
        "\n",
        "def generate_captions(num_frames):\n",
        "    captions = []\n",
        "    for i in range(num_frames):\n",
        "        img = Image.open(f\"frames/frame_{i:04d}.jpg\").convert(\"RGB\")\n",
        "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "        output = blip_model.generate(**inputs)\n",
        "        caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "    return captions\n",
        "\n",
        "def transcribe_audio(video_path):\n",
        "    clip = VideoFileClip(video_path)\n",
        "    audio_path = \"audio.wav\"\n",
        "    clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "    result = whisper_model.transcribe(audio_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "def summarize_text(transcription, captions):\n",
        "    document = \"Transcription: \" + transcription + \"\\n\" + \"Visual description: \" + \" \".join(captions)\n",
        "    inputs = bart_tokenizer.encode(document, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def create_summary_video(summary, num_frames=10):\n",
        "    tts = gTTS(text=summary, lang=\"en\")\n",
        "    tts_path = \"summary_audio.mp3\"\n",
        "    tts.save(tts_path)\n",
        "    audio_clip = AudioFileClip(tts_path)\n",
        "    audio_duration = audio_clip.duration\n",
        "    frame_files = sorted([os.path.join(\"frames\", f) for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])[:num_frames]\n",
        "    if not frame_files:\n",
        "        raise Exception(\"Aucune image trouvée.\")\n",
        "    frame_duration = audio_duration / len(frame_files)\n",
        "    video_clip = ImageSequenceClip(frame_files, durations=[frame_duration]*len(frame_files))\n",
        "    final_clip = video_clip.set_audio(audio_clip)\n",
        "    output_path = \"video_resumee_synced.mp4\"\n",
        "    final_clip.write_videofile(output_path, fps=1, codec=\"libx264\", verbose=False, logger=None)\n",
        "    return output_path\n",
        "\n",
        "def cleanup():\n",
        "    shutil.rmtree(\"frames\", ignore_errors=True)\n",
        "    for f in [\"audio.wav\", \"summary_audio.mp3\"]:\n",
        "        if os.path.exists(f): os.remove(f)\n",
        "\n",
        "def generate_summary(video_path):\n",
        "    num_frames = extract_frames(video_path)\n",
        "    captions = generate_captions(num_frames)\n",
        "    transcription = transcribe_audio(video_path)\n",
        "    summary = summarize_text(transcription, captions)\n",
        "    summary_video_path = create_summary_video(summary, num_frames=min(num_frames, 10))\n",
        "    cleanup()\n",
        "    return summary_video_path\n",
        "\n",
        "# Route pour uploader la vidéo et recevoir la vidéo résumée\n",
        "@app.route('/upload_video', methods=['POST'])\n",
        "def upload_video():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    video_path = f\"uploaded_{file.filename}\"\n",
        "    file.save(video_path)\n",
        "\n",
        "    try:\n",
        "        summary_video_path = generate_summary(video_path)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "    finally:\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)\n",
        "\n",
        "    return send_file(summary_video_path, mimetype='video/mp4')\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def index():\n",
        "    return jsonify({\"message\": \"API fonctionne\", \"endpoint\": \"/upload_video\"})\n",
        "\n",
        "# Démarrage de ngrok pour exposer l'API publiquement\n",
        "def start_ngrok():\n",
        "    # Met ici ton token ngrok personnel, si tu en as un\n",
        "    os.environ[\"NGROK_AUTH_TOKEN\"] = \"2rRtD0JOoMbRrKsgLY5gIsC418i_3gtqQqcw6Z4QimS2uS8R5\"\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(\"✅ API publique ngrok :\", public_url)\n",
        "\n",
        "def start_flask():\n",
        "    app.run()\n",
        "\n",
        "# Lancement ngrok + flask simultanément\n",
        "start_ngrok()\n",
        "Thread(target=start_flask).start()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "TOar1XZJQGBF",
        "outputId": "22eb2c96-c511-4460-b270-3c5e82781b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-2-9813a36c5caa>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-9813a36c5caa>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ngrok config add-authtoken $YOUR_AUTHTOKEN\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installer les dépendances (à faire une seule fois)\n",
        "!pip install flask transformers pillow torch torchvision torchaudio opencv-python moviepy gtts git+https://github.com/openai/whisper.git --quiet\n",
        "\n",
        "# Imports\n",
        "from flask import Flask, request, send_file, jsonify\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, BartTokenizer, BartForConditionalGeneration\n",
        "import whisper\n",
        "from moviepy.editor import VideoFileClip, ImageSequenceClip, AudioFileClip\n",
        "from gtts import gTTS\n",
        "\n",
        "# Création app Flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Device GPU si dispo\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Chargement des modèles (une seule fois)\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
        "\n",
        "# Extraction des frames (1 frame par seconde)\n",
        "def extract_frames(video_path, fps_extract=1):\n",
        "    os.makedirs(\"frames\", exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    count, saved = 0, 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % fps == 0:\n",
        "            path = f\"frames/frame_{saved:04d}.jpg\"\n",
        "            cv2.imwrite(path, frame)\n",
        "            saved += 1\n",
        "        count += 1\n",
        "    cap.release()\n",
        "    return saved\n",
        "\n",
        "# Générer captions pour chaque frame\n",
        "def generate_captions(num_frames):\n",
        "    captions = []\n",
        "    for i in range(num_frames):\n",
        "        img = Image.open(f\"frames/frame_{i:04d}.jpg\").convert(\"RGB\")\n",
        "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "        output = blip_model.generate(**inputs)\n",
        "        caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "    return captions\n",
        "\n",
        "# Transcrire audio de la vidéo avec Whisper\n",
        "def transcribe_audio(video_path):\n",
        "    clip = VideoFileClip(video_path)\n",
        "    audio_path = \"audio.wav\"\n",
        "    clip.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "    result = whisper_model.transcribe(audio_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Résumer texte avec BART\n",
        "def summarize_text(transcription, captions):\n",
        "    document = \"Transcription: \" + transcription + \"\\n\" + \"Visual description: \" + \" \".join(captions)\n",
        "    inputs = bart_tokenizer.encode(document, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    summary_ids = bart_model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4)\n",
        "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Créer vidéo résumée avec audio TTS\n",
        "def create_summary_video(summary, num_frames=10):\n",
        "    tts = gTTS(text=summary, lang=\"en\")\n",
        "    tts_path = \"summary_audio.mp3\"\n",
        "    tts.save(tts_path)\n",
        "    audio_clip = AudioFileClip(tts_path)\n",
        "    audio_duration = audio_clip.duration\n",
        "    frame_files = sorted([os.path.join(\"frames\", f) for f in os.listdir(\"frames\") if f.endswith(\".jpg\")])[:num_frames]\n",
        "    if not frame_files:\n",
        "        raise Exception(\"No frames found.\")\n",
        "    frame_duration = audio_duration / len(frame_files)\n",
        "    video_clip = ImageSequenceClip(frame_files, durations=[frame_duration]*len(frame_files))\n",
        "    final_clip = video_clip.set_audio(audio_clip)\n",
        "    output_path = \"video_resumee_synced.mp4\"\n",
        "    final_clip.write_videofile(output_path, fps=1, codec=\"libx264\", verbose=False, logger=None)\n",
        "    return output_path\n",
        "\n",
        "# Nettoyage fichiers temporaires\n",
        "def cleanup():\n",
        "    shutil.rmtree(\"frames\", ignore_errors=True)\n",
        "    for f in [\"audio.wav\", \"summary_audio.mp3\"]:\n",
        "        if os.path.exists(f): os.remove(f)\n",
        "\n",
        "# Fonction principale résumé vidéo\n",
        "def generate_summary(video_path):\n",
        "    num_frames = extract_frames(video_path)\n",
        "    captions = generate_captions(num_frames)\n",
        "    transcription = transcribe_audio(video_path)\n",
        "    summary = summarize_text(transcription, captions)\n",
        "    summary_video_path = create_summary_video(summary, num_frames=min(num_frames, 10))\n",
        "    cleanup()\n",
        "    return summary_video_path\n",
        "\n",
        "# Route upload vidéo -> vidéo résumée\n",
        "@app.route('/upload_video', methods=['POST'])\n",
        "def upload_video():\n",
        "    if 'file' not in request.files:\n",
        "        return jsonify({\"error\": \"No file part\"}), 400\n",
        "    file = request.files['file']\n",
        "    if file.filename == '':\n",
        "        return jsonify({\"error\": \"No selected file\"}), 400\n",
        "\n",
        "    video_path = f\"uploaded_{file.filename}\"\n",
        "    file.save(video_path)\n",
        "\n",
        "    try:\n",
        "        summary_video_path = generate_summary(video_path)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "    finally:\n",
        "        if os.path.exists(video_path):\n",
        "            os.remove(video_path)\n",
        "\n",
        "    return send_file(summary_video_path, mimetype='video/mp4')\n",
        "\n",
        "@app.route(\"/\", methods=[\"GET\"])\n",
        "def index():\n",
        "    return jsonify({\"message\": \"API fonctionne\", \"endpoint\": \"/upload_video\"})\n",
        "\n",
        "# Lancer Flask (serveur local)\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOWdMcvIP6ud",
        "outputId": "ed14cca5-f05d-44ee-d522-b1a7c29be343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Using device: cpu\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2rRtD0JOoMbRrKsgLY5gIsC418i_3gtqQqcw6Z4QimS2uS8R5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2s18Bg_RHpp",
        "outputId": "d6b2ee3a-b470-4392-add9-a225300e7df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    }
  ]
}